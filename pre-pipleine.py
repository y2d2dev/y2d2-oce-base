import os
import sys
import yaml
import json
import logging
import shutil
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.pipeline.pdf_processor import PDFProcessor
from src.pipeline.image_splitter import ImageSplitter
from src.pipeline.llm_evaluator import LLMEvaluator
from src.dewarping.dewarping_runner import DewarpingRunner
from src.super_resolution.sr_runner import SuperResolutionRunner
from src.utils.logger import setup_logger
from src.utils.file_utils import ensure_directory, cleanup_directory
from src.utils.image_utils import split_image_left_right_with_overlap, rotate_image_correction
from src.utils.orientation_detector import OrientationDetector
import cv2
import torch

logger = logging.getLogger(__name__)


class DocumentOCRPipeline:
    """
    æ›¸é¡OCRå‰å‡¦ç†ã®çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 

    process_pdf ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚Šï¼Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹

    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. PDF â†’ JPGå¤‰æ› (DPIè‡ªå‹•èª¿æ•´)
    2-1. ç”»åƒã®æ­ªã¿(ãŠã‚ˆã³è­˜åˆ¥å›°é›£æ€§ã®åˆ¤å®š) (LLM)
    2-2. æœ€é«˜è§£åƒåº¦åŒ– (å¿…è¦ãªå ´åˆ)
    2-3. æ­ªã¿è£œæ­£ (å¿…è¦ãªå ´åˆ)
    3-1. å›è»¢åˆ¤å®š (LLM)
    3-2. å›è»¢è£œæ­£
    4-1. ãƒšãƒ¼ã‚¸æ•°ç­‰åˆ¤å®š (LLM)
    4-2. ãƒšãƒ¼ã‚¸åˆ†å‰² (å¿…è¦ãªå ´åˆ)
    5-1. ç”»åƒ5ç­‰åˆ† (ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ã)
    6-1. è¶…è§£åƒå‡¦ç† (DRCT)
    7-1. OCRå®Ÿè¡Œ (LLM)
    """
    
    def __init__(self, config_path: str, processing_options: Optional[Dict] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        
        Args:
            config_path (str): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            processing_options (Optional[Dict]): å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                - skip_super_resolution (bool): è¶…è§£åƒå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                - skip_ocr (bool): OCRå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        """
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self._load_env()
        
        self.config_path = config_path
        self.processing_options = processing_options or {}
        self.config = self._load_config()
        self._apply_processing_options()
        self._setup_logging()
        self._initialize_components()
        self._setup_directories()
        
        logger.info("ğŸ‰ DocumentOCRPipeline åˆæœŸåŒ–å®Œäº†")

    @staticmethod
    def _to_bool(v) -> bool:
        if isinstance(v, bool):
            return v
        if v is None:
            return False
        if isinstance(v, (int, float)):
            return v != 0
        if isinstance(v, str):
            s = v.strip().lower()
            if s in ("true"):
                return True
            if s in ("false"):
                return False
        return False

    @staticmethod
    def _to_int(v, default: Optional[int] = None) -> Optional[int]:
        if v is None:
            return default
        if isinstance(v, bool):
            return 1 if v else 0
        if isinstance(v, int):
            return v
        try:
            if isinstance(v, float):
                return int(v)
            s = str(v).strip()
            if s == "":
                return default
            return int(float(s))
        except Exception:
            return default

    @staticmethod
    def _to_float(v, default: Optional[float] = None) -> Optional[float]:
        if v is None:
            return default
        if isinstance(v, bool):
            return 1.0 if v else 0.0
        if isinstance(v, (int, float)):
            return float(v)
        try:
            s = str(v).strip()
            if s == "":
                return default
            return float(s)
        except Exception:
            return default
 
    def _load_env(self):
        """
        .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
        """
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        current_dir = Path(__file__).parent.parent.parent  # src/pipeline/ ã‹ã‚‰ project root ã¸
        env_path = current_dir / '.env'
        
        if env_path.exists():
            load_dotenv(env_path)
            logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
        else:
            logger.warning(f".envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
    
    def _load_config(self) -> Dict:
        """
        è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            Dict: è¨­å®šãƒ‡ãƒ¼ã‚¿
        """
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è¨­å®šã®ä¸Šæ›¸ã
            if 'GEMINI_API_KEY' in os.environ:
                config.setdefault('llm_evaluation', {})['api_key'] = os.environ['GEMINI_API_KEY']
            
            return config
            
        except Exception as e:
            raise RuntimeError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _apply_processing_options(self):
        """
        å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®šã«é©ç”¨
        """
        if self.processing_options.get('skip_super_resolution'):
            # è¶…è§£åƒè¨­å®šã‚’ç„¡åŠ¹åŒ–
            if 'super_resolution' not in self.config:
                self.config['super_resolution'] = {}
            self.config['super_resolution']['enabled'] = False
            logger.info("âš¡ è¶…è§£åƒå‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
        
        if self.processing_options.get('skip_dewarping'):
            # æ­ªã¿è£œæ­£è¨­å®šã‚’ç„¡åŠ¹åŒ–
            if 'dewarping' not in self.config:
                self.config['dewarping'] = {}
            self.config['dewarping']['enabled'] = False
            logger.info("âš¡ æ­ªã¿è£œæ­£å‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
        
        if self.processing_options.get('skip_ocr'):
            # OCRè¨­å®šã‚’ç„¡åŠ¹åŒ–
            if 'llm_evaluation' not in self.config:
                self.config['llm_evaluation'] = {}
            self.config['llm_evaluation']['ocr_enabled'] = False
            logger.info("âš¡ OCRå‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
    
    def _setup_logging(self):
        """
        éšå±¤æ§‹é€ ã‚’æŒã¤ã‚¹ãƒãƒ¼ãƒˆãªãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š
        """
        log_level = self.config.get('system', {}).get('log_level', 'INFO')
        
        # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã®è¨­å®šã‚’å¼·åˆ¶çš„ã«è¡Œã†
        import logging
        import sys
        
        # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        root_logger = logging.getLogger()
        root_logger.handlers.clear()
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ
        class HierarchicalFormatter(logging.Formatter):
            """éšå±¤æ§‹é€ ã‚’è¡¨ç¾ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼"""
            
            def __init__(self):
                super().__init__()
                self.component_prefixes = {
                    'src.pipeline.main_pipeline_v2': 'ğŸš€',
                    'src.pipeline.pdf_processor': 'ğŸ“„',
                    'src.dewarping.dewarping_runner': 'ğŸ”§',
                    'src.super_resolution.sr_runner': 'ğŸ”',
                    'src.pipeline.image_splitter': 'âœ‚ï¸',
                    'src.pipeline.llm_evaluator': 'ğŸ¤–',
                }
            
            def format(self, record):
                # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåã‚’å–å¾—
                component_name = record.name
                message = record.getMessage()
                
                # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
                prefix = None
                is_main_component = component_name.startswith('src.pipeline.main_pipeline_v2')
                
                for module_name, module_prefix in self.component_prefixes.items():
                    if component_name.startswith(module_name):
                        if is_main_component:
                            # main_pipelineã®å ´åˆã¯ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ã§ã‚·ãƒ³ãƒ—ãƒ«ã«
                            prefix = None
                        else:
                            # ã‚µãƒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã§è¡¨ç¤º
                            prefix = f"  {module_prefix}"
                        break
                
                # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸè£…é£¾
                if record.levelno >= logging.ERROR:
                    level_icon = 'âŒ '
                elif record.levelno >= logging.WARNING:
                    level_icon = 'âš ï¸ '
                else:
                    level_icon = ''
                
                # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                if prefix:
                    return f"{prefix} {level_icon}{message}"
                else:
                    return f"{level_icon}{message}"
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(getattr(logging, log_level.upper()))
        console_handler.setFormatter(HierarchicalFormatter())
        
        root_logger.addHandler(console_handler)
        root_logger.setLevel(getattr(logging, log_level.upper()))
        
        # å­ãƒ­ã‚¬ãƒ¼ã®ä¼æ’­ã‚’æœ‰åŠ¹ã«ã—ã¦çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é©ç”¨
        for logger_name in ['src.pipeline', 'src.dewarping', 'src.super_resolution']:
            child_logger = logging.getLogger(logger_name)
            child_logger.propagate = True
            
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§é‡è¤‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        class SuppressFilter(logging.Filter):
            def filter(self, record):
                # main_pipelineã‹ã‚‰ã®ç‰¹å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚µãƒ—ãƒ¬ã‚¹
                if record.name.startswith('src.pipeline.main_pipeline'):
                    message = record.getMessage()
                    suppressed_patterns = [
                        'LLMæ­ªã¿åˆ¤å®š',
                        'æ­ªã¿è£œæ­£å‡¦ç†',
                        'è¶…è§£åƒå‡¦ç†é–‹å§‹',
                    ]
                    for pattern in suppressed_patterns:
                        if pattern in message:
                            return False
                return True
                
        console_handler.addFilter(SuppressFilter())
        
        # å€‹åˆ¥ãƒ­ã‚¬ãƒ¼ã‚‚è¨­å®šï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        setup_logger(level=log_level)
    
    def _initialize_components(self):
        """
        å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆè¨­å®šè¾æ›¸æ¸¡ã—æ–¹å¼ï¼‰
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã®èª­ã¿è¾¼ã¿
        self.prompts = self._load_prompts()

        # PDFå‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        pdf_config = self.config.get('pdf_processing', {})
        self.pdf_processor = PDFProcessor(config=pdf_config)

        # LLMè©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆconfig
        llm_config = self.config.get('llm_evaluation', {})
        
        # LLMEvaluatorã®åˆæœŸåŒ–
        ## æ­ªã¿åˆ¤å®šç”¨LLMEvaluator
        judgment_config = llm_config.get('judgment', llm_config)
        self.llm_evaluator_judgment = LLMEvaluator(config=judgment_config)

        # OCRç”¨LLMEvaluator
        ocr_config = llm_config.get('ocr', llm_config)
        self.llm_evaluator_ocr = LLMEvaluator(config=ocr_config)

        # å›è»¢æ¤œå‡ºãŠã‚ˆã³è£œæ­£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        # è¨­å®šã‚­ãƒ¼ã®å¾Œæ–¹äº’æ›æ€§: rotation | orientation_detection
        orient_cfg = self.config.get('orientation_detection', self.config.get('rotation', {}))
        self.orientation_detector = OrientationDetector(config=orient_cfg)
        
        # Orientation ç”¨ã® LLM (å°‚ç”¨è¨­å®šãŒã‚ã‚Œã°ä½¿ç”¨)
        orientation_llm_cfg = llm_config.get('orientation_judgment', llm_config)
        self.llm_evaluator_orientation = LLMEvaluator(config=orientation_llm_cfg)
        # OrientationDetector ã« LLM ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–¢é€£ä»˜ã‘
        try:
            self.orientation_detector.attach_llm_evaluator(self.llm_evaluator_orientation, self.prompts)
        except Exception as e:
            logger.warning(f"OrientationDetector ã® LLM æ·»ä»˜ã«å¤±æ•—: {e}")

        # æ­ªã¿è£œæ­£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        dewarping_config = self.config.get('dewarping', {})
        self.dewarping_runner = DewarpingRunner(config=dewarping_config)

        # ç”»åƒåˆ†å‰²ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        split_config = self.config.get('split_image_for_ocr', {})
        self.image_splitter = ImageSplitter(config=split_config)

        # è¶…è§£åƒã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        sr_config = self.config.get('super_resolution', {})
        self.sr_runner = SuperResolutionRunner(config=sr_config)

    def _load_prompts(self) -> Dict:
        """
        LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            Dict: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šãƒ‡ãƒ¼ã‚¿
        """
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®llm_prompts.yamlã‚’æ¢ã™
            config_dir = os.path.dirname(os.path.abspath(self.config_path))
            prompts_path = os.path.join(config_dir, 'llm_prompts.yaml')
            logger.debug(f"_load_prompts: config_dir={config_dir}")
            logger.debug(f"_load_prompts: initial prompts_path={prompts_path}")
            
            if not os.path.exists(prompts_path):
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®configãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ç¢ºèª
                project_root = Path(__file__).parent.parent.parent
                fallback_path = project_root / "config" / "llm_prompts.yaml"
                logger.debug(f"_load_prompts: project_root={project_root}")
                logger.debug(f"_load_prompts: fallback_path={fallback_path}")
                if fallback_path.exists():
                    prompts_path = str(fallback_path)
                    logger.debug(f"_load_prompts: using fallback_path={prompts_path}")
                else:
                    raise FileNotFoundError(f"llm_prompts.yaml not found in {config_dir} or {fallback_path}")
            
            with open(prompts_path, 'r', encoding='utf-8') as f:
                prompts = yaml.safe_load(f)
                logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šèª­ã¿è¾¼ã¿: {prompts_path}")
                return prompts
                
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}", exc_info=True)
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
            raise RuntimeError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _setup_directories(self):
        """
        ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        """
        self.dirs = self.config.get('directories', {})
        
        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        for dir_key, dir_path in self.dirs.items():
            ensure_directory(dir_path)
            logger.debug(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª: {dir_key} -> {dir_path}")

    def _create_session_directories(self, session_id: str) -> Dict[str, str]:
        """
        ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        
        Args:
            session_id (str): ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            
        Returns:
            Dict[str, str]: ä½œæˆã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        base_output = self.dirs.get("output", "data/output")
        session_dirs = {}
        
        dir_names = [
            "converted_images", "llm_judgments", "dewarped", 
            "split_images", "super_resolved", "final_results"
        ]
        
        for dir_name in dir_names:
            dir_path = os.path.join(base_output, dir_name, session_id)
            ensure_directory(dir_path)
            session_dirs[dir_name] = dir_path
        
        return session_dirs    
  
    def _pdf_to_jpg(self, pdf_path: str, output_dir: str) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—1: PDF â†’ JPGå¤‰æ›
        
        Args:
            pdf_path (str): PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            Dict: å¤‰æ›çµæœ
        """
        try:
            result = self.pdf_processor.process_pdf(pdf_path, output_dir)
            logger.info(f"PDFå¤‰æ›å®Œäº†: {result['page_count']} ãƒšãƒ¼ã‚¸")
            return result
            
        except Exception as e:
            logger.error(f"PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def _dewarping_llm_judgment(self, image_path: str, output_dir: str, page_number: int) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—2-1: LLMæ­ªã¿åˆ¤å®š&å†ç”»åƒåŒ–åˆ¤å®š
        
        Args:
            image_path (str): åˆ¤å®šå¯¾è±¡ç”»åƒ
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            page_number (int): ãƒšãƒ¼ã‚¸ç•ªå·
            
        Returns:
            Dict: åˆ¤å®šçµæœ
        """
        
        try:
            prompts = self.prompts.get("dewarping_judgment", {})
            result = self.llm_evaluator_judgment.evaluate_dewarping_need(image_path, prompts)
            
            # çµæœã‚’ä¿å­˜
            if result["success"]:
                output_file = os.path.join(output_dir, f"page_{page_number:03d}_dewarping_judgment.json")
                self.llm_evaluator_judgment.save_result(result, output_file)
            
            return result
            
        except Exception as e:
            logger.error(f"LLMåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def _reprocess_page_from_pdf_at_scale(self, 
                                        page_number: int, 
                                        scale_factor: float, 
                                        output_dir: str) -> Dict:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã‚’PDFã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã§å†ç”»åƒåŒ–ã™ã‚‹ã€‚
        
        Args:
            page_number (int): å†å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1ãƒ™ãƒ¼ã‚¹ï¼‰ã€‚
            scale_factor (float): å…ƒã®DPIã«å¯¾ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã€‚ï¼ˆä¾‹: 2.0ã§2å€ã€4.0ã§4å€ï¼‰
            output_dir (str): å†ç”»åƒåŒ–ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
            
        Returns:
            Dict: å†ç”»åƒåŒ–ã®çµæœï¼ˆæˆåŠŸ/å¤±æ•—ã€å‡ºåŠ›ãƒ‘ã‚¹ãªã©ï¼‰ã€‚
        """
        logger.info(f"ğŸ”„ ãƒšãƒ¼ã‚¸ {page_number} ã‚’PDFã‹ã‚‰ {scale_factor}x ã‚¹ã‚±ãƒ¼ãƒ«ã§å†ç”»åƒåŒ–ã—ã¾ã™ã€‚")
        
        if not hasattr(self, '_current_pdf_path') or not self._current_pdf_path:
            logger.error("ç¾åœ¨ã®PDFãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†ç”»åƒåŒ–ã§ãã¾ã›ã‚“ã€‚")
            return {"success": False, "error": "PDFãƒ‘ã‚¹æœªè¨­å®š"}
        
        if not hasattr(self, '_current_pdf_info') or not self._current_pdf_info:
            logger.error("PDFæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†ç”»åƒåŒ–ã§ãã¾ã›ã‚“ã€‚")
            return {"success": False, "error": "PDFæƒ…å ±æœªè¨­å®š"}

        try:
            # 0ãƒ™ãƒ¼ã‚¹ã®ãƒšãƒ¼ã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            page_idx = page_number - 1
            
            # å…ƒã®PDFæƒ…å ±ã‹ã‚‰è©²å½“ãƒšãƒ¼ã‚¸ã®DPIã¨ã‚µã‚¤ã‚ºã‚’å–å¾—
            original_page_info = None
            for p_info in self._current_pdf_info["pages"]:
                if p_info["page_number"] == page_number:
                    original_page_info = p_info
                    break
            
            if not original_page_info:
                logger.error(f"ãƒšãƒ¼ã‚¸ {page_number} ã®å…ƒã®PDFæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return {"success": False, "error": "ãƒšãƒ¼ã‚¸æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

            original_dpi = original_page_info["used_dpi"]
            new_dpi = int(original_dpi * scale_factor)
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            os.makedirs(output_dir, exist_ok=True)
            
            # æ–°ã—ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
            base_name = os.path.splitext(os.path.basename(self._current_pdf_path))[0]
            image_filename = f"{base_name}_page_{page_number:03d}_scaled_{int(scale_factor)}x.jpg"
            output_image_path = os.path.join(output_dir, image_filename)
            
            # PDFProcessorã‚’ä½¿ç”¨ã—ã¦å†å¤‰æ›
            converted_path = self.pdf_processor.convert_page_to_image(
                self._current_pdf_path, 
                page_idx, 
                new_dpi, 
                output_image_path
            )
            
            if converted_path and os.path.exists(converted_path):
                logger.info(f"âœ… ãƒšãƒ¼ã‚¸ {page_number} ã‚’ {new_dpi} DPIã§å†ç”»åƒåŒ–æˆåŠŸ: {converted_path}")
                return {
                    "success": True,
                    "original_image_path": original_page_info["image_file"],
                    "reprocessed_image_path": converted_path,
                    "new_dpi": new_dpi,
                    "scale_factor": scale_factor
                }
            else:
                logger.error(f"âŒ ãƒšãƒ¼ã‚¸ {page_number} ã®å†ç”»åƒåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return {"success": False, "error": "å†ç”»åƒåŒ–å¤±æ•—"}

        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸ {page_number} ã®å†ç”»åƒåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return {"success": False, "error": str(e)}

    def _apply_reprocess_page(self, page_judgments: List[Dict], session_dirs: Dict[str, str]):
        """ã‚¹ãƒ†ãƒƒãƒ—2-2: èª­ã¿ã«ãã•ãŒ major ã®ãƒšãƒ¼ã‚¸ã‚’PDFã‹ã‚‰2xã§å†ç”»åƒåŒ–ã—ã€processed_imageç¾¤ã‚’æ›´æ–°"""
        if not page_judgments:
            return
        try:
            reproc_dir = os.path.join(session_dirs.get("converted_images", ""), "reprocessed_scaled")
            os.makedirs(reproc_dir, exist_ok=True)
        except Exception:
            reproc_dir = session_dirs.get("converted_images", "")
        for i, page_data in enumerate(page_judgments, 1):
            if page_data.get("skip_processing"):
                continue
            pn = page_data.get("page_number")
            readability_issues = str(page_data.get("readability_issues", "")).lower()
            # åˆæœŸå€¤
            if "reprocessed_at_scale" not in page_data:
                page_data["reprocessed_at_scale"] = False
            if readability_issues == "major":
                logger.info(f"  â†’ ãƒšãƒ¼ã‚¸ {pn}: èª­ã¿ã«ãã•ã€majorã€ã®ãŸã‚ã€2å€ã‚¹ã‚±ãƒ¼ãƒ«ã§å†ç”»åƒåŒ–ã‚’è©¦è¡Œ")
                try:
                    rep = self._reprocess_page_from_pdf_at_scale(pn, 2.0, reproc_dir)
                    # çµæœã®åæ˜ 
                    if rep.get("success"):
                        new_img = rep["reprocessed_image_path"]
                        page_data["processed_image"] = new_img
                        page_data["processed_images"] = [new_img]
                        page_data["reprocessed_at_scale"] = True
                        page_data["reprocess_result"] = rep
                        logger.info("  â†’ 2å€ã‚¹ã‚±ãƒ¼ãƒ«ã§å†ç”»åƒåŒ–æˆåŠŸã€‚æ–°ã—ã„ç”»åƒã‚’ä½¿ç”¨")
                    else:
                        page_data["reprocess_result"] = rep
                        logger.warning("  â†’ 2å€ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å†ç”»åƒåŒ–å¤±æ•—ã€‚å…ƒç”»åƒã‚’ä½¿ç”¨")
                except Exception as e:
                    logger.warning(f"  â†’ å†ç”»åƒåŒ–å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    page_data["reprocessed_at_scale"] = False
            else:
                # major ä»¥å¤–ã¯ä½•ã‚‚ã—ãªã„
                continue
    
    def _step_dewarping(self, image_path: str, output_dir: str, page_number: int) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—2-2: æ­ªã¿è£œæ­£
        
        Args:
            image_path (str): è£œæ­£å¯¾è±¡ç”»åƒ
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            page_number (int): ãƒšãƒ¼ã‚¸ç•ªå·
            
        Returns:
            Dict: è£œæ­£çµæœ
        """
        # æ­ªã¿è£œæ­£å‡¦ç†ï¼ˆè©³ç´°ã¯DewarpingRunnerã§å‡ºåŠ›ï¼‰
        
        try:
            output_file = os.path.join(output_dir, f"page_{page_number:03d}_dewarped.jpg")
            result = self.dewarping_runner.process_image(image_path, output_file)
            return result
            
        except Exception as e:
            logger.error(f"æ­ªã¿è£œæ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def _apply_dewarping(self, page_judgments: List[Dict], session_dirs: Dict[str, str]):
        """
        ã‚¹ãƒ†ãƒƒãƒ—2-2: æ­ªã¿è£œæ­£
        """
        # æ­ªã¿è£œæ­£ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if not self.config.get('dewarping', {}).get('enabled', True):
            logger.info("âš¡ æ­ªã¿è£œæ­£å‡¦ç†ãŒè¨­å®šã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
            # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã™çµæœã‚’å„ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã«è¨­å®š
            for page_data in page_judgments:
                # processed_images ãŒå­˜åœ¨ã—ãªã„å ´åˆã€original_image ã‚’ä½¿ç”¨
                if "processed_images" not in page_data or not page_data["processed_images"]:
                    page_data["processed_images"] = [page_data["processed_image"]]
                page_data["dewarping_result"] = {"success": True, "skipped": True, "reason": "dewarping_disabled_by_config"}
                logger.info(f"  â†’ ãƒšãƒ¼ã‚¸ {page_data['page_number']}: æ­ªã¿è£œæ­£ã‚¹ã‚­ãƒƒãƒ— (è¨­å®šã«ã‚ˆã‚‹)")
            return

        dewarping_needed_pages = [p for p in page_judgments if p.get("needs_dewarping") and not p.get("skip_processing")]
        if not dewarping_needed_pages:
            return

        logger.info(f"ğŸ”§ æ­ªã¿è£œæ­£å‡¦ç†: {len(dewarping_needed_pages)}ãƒšãƒ¼ã‚¸å¯¾è±¡")
        for i, page_data in enumerate(dewarping_needed_pages, 1):
            page_number = page_data["page_number"]
            image_path = page_data["processed_image"]
            page_count = page_data.get("page_count", 1)
            
            # 3ãƒšãƒ¼ã‚¸ã®å ´åˆã¯å¼·åˆ¶æ­ªã¿è£œæ­£ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º
            if page_count >= 3:
                logger.info(f"ğŸ”§ æ­ªã¿è£œæ­£ ({i}/{len(dewarping_needed_pages)}) ãƒšãƒ¼ã‚¸{page_number} [3ãƒšãƒ¼ã‚¸åˆ¤å®šã«ã‚ˆã‚‹å¼·åˆ¶è£œæ­£]")
            else:
                logger.info(f"ğŸ”§ æ­ªã¿è£œæ­£ ({i}/{len(dewarping_needed_pages)}) ãƒšãƒ¼ã‚¸{page_number}")
            dewarping_result = self._step_dewarping(image_path, session_dirs["dewarped"], page_number)
            page_data["dewarping_result"] = dewarping_result
            
            if dewarping_result["success"] and not dewarping_result.get("skipped", False) and dewarping_result.get("output_paths"):
                page_data["processed_images"] = dewarping_result["output_paths"]
                page_data["processed_image"] = dewarping_result["output_paths"][0]
                
                # 3ãƒšãƒ¼ã‚¸åˆ¤å®šã§æ­ªã¿è£œæ­£ã—ãŸå ´åˆã®çµæœã‚’æ˜ç¤º
                if page_count >= 3:
                    num_images = len(dewarping_result['output_paths'])
                    if num_images == 1:
                        logger.info(f"  â†’ âœ…è£œæ­£å®Œäº†: 1ãƒšãƒ¼ã‚¸ã«åˆ†å‰²ï¼ˆ3ãƒšãƒ¼ã‚¸åˆ¤å®šâ†’1ãƒšãƒ¼ã‚¸ã«è£œæ­£ï¼‰")
                    elif num_images == 2:
                        logger.info(f"  â†’ âœ…è£œæ­£å®Œäº†: 2ãƒšãƒ¼ã‚¸ã«åˆ†å‰²ï¼ˆ3ãƒšãƒ¼ã‚¸åˆ¤å®šâ†’2ãƒšãƒ¼ã‚¸ã«è£œæ­£ï¼‰")
                    else:
                        logger.info(f"  â†’ âœ…è£œæ­£å®Œäº†: {num_images}å€‹ã®ç”»åƒç”Ÿæˆ")
                else:
                    logger.info(f"  â†’ âœ…è£œæ­£å®Œäº†: {len(dewarping_result['output_paths'])}å€‹ã®ç”»åƒç”Ÿæˆ")
            else:
                logger.info(f"  â†’ âš ï¸è£œæ­£ã‚¹ã‚­ãƒƒãƒ—/å¤±æ•— (å‰å‡¦ç†ç”»åƒä½¿ç”¨)")

            if hasattr(self, 'dewarping_runner') and torch.cuda.is_available():
                torch.cuda.empty_cache()

    def _apply_orientation_detector(self, page_judgments: List[Dict], session_dirs: Dict[str, str]):
        """
        ã‚¹ãƒ†ãƒƒãƒ—3-1: å›è»¢åˆ¤å®šãŠã‚ˆã³è£œæ­£
        """
        if not page_judgments:
            return
        # ç”Ÿæˆç‰©ã®ä¿å­˜å…ˆï¼ˆãƒ‡ãƒãƒƒã‚°å¯¾æ¯”ç”¨ï¼‰: llm_judgmentsé…ä¸‹ã«é›†ç´„
        try:
            root_debug_dir = session_dirs.get("llm_judgments", session_dirs["converted_images"])
            os.makedirs(root_debug_dir, exist_ok=True)
            # OrientationDetector å´ã®ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆï¼ˆorientation_pairs, orientation_debugã‚’è‡ªå‹•ç®¡ç†ï¼‰
            self.orientation_detector.debug_save_dir = root_debug_dir
        except Exception:
            pass

        logger.info("ğŸ§­ å›è»¢åˆ¤å®šãŠã‚ˆã³è£œæ­£ã‚’é©ç”¨")
        for i, page_data in enumerate(page_judgments, 1):
            if page_data.get("skip_processing"):
                continue
            page_number = page_data["page_number"]
            proc_images = page_data.get("processed_images") or [page_data.get("processed_image")]
            if not proc_images:
                continue
            new_paths: List[str] = []
            for img_idx, img_path in enumerate(proc_images):
                try:

                    det = self.orientation_detector.detect(img_path, add_star=True, temp_dir=None, use_llm=True) # ã“ã“ã§ LLM ã‚’ä½¿ç”¨ã—ã¦å›è»¢æ–¹å‘ã‚’æ¤œå‡º
                    angle = det.angle
                    if angle == 0:
                        logger.info(f"  â†ªï¸ ãƒšãƒ¼ã‚¸{page_number} ç”»åƒ{img_idx+1}: å›è»¢ä¸è¦")
                        new_paths.append(img_path)
                        continue
                    # ç”»åƒã‚’å›è»¢ã—ã¦ä¿å­˜
                    img = cv2.imread(img_path)
                    if img is None:
                        logger.warning(f"  â†ªï¸ ãƒšãƒ¼ã‚¸{page_number} ç”»åƒ{img_idx+1}: ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•— (å›è»¢ã‚¹ã‚­ãƒƒãƒ—)")
                        new_paths.append(img_path)
                        continue
                    if angle == 90:
                        rotated = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)
                    elif angle == -90:
                        rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)
                    elif angle in (180, -180):
                        rotated = cv2.rotate(img, cv2.ROTATE_180)
                    else:
                        rotated = img
                    base, ext = os.path.splitext(img_path)
                    out_path = f"{base}_rot{ext or '.jpg'}"
                    cv2.imwrite(out_path, rotated)
                    logger.info(f"  â†ªï¸ ãƒšãƒ¼ã‚¸{page_number} ç”»åƒ{img_idx+1}: {angle}åº¦å›è»¢ â†’ {os.path.basename(out_path)}")
                    new_paths.append(out_path)
                except Exception as e:
                    logger.warning(f"  â†ªï¸ ãƒšãƒ¼ã‚¸{page_number} ç”»åƒ{img_idx+1}: å›è»¢å‡¦ç†ã‚¨ãƒ©ãƒ¼ {e}")
                    new_paths.append(img_path)
            # æ›´æ–°
            page_data["processed_images"] = new_paths
            page_data["processed_image"] = new_paths[0]

    def _page_count_etc_llm_judgment(self, image_path: str, output_dir: str, page_number: int, image_index: Optional[int] = None) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—4-1: ãƒšãƒ¼ã‚¸æ•°ç­‰åˆ¤å®šï¼ˆLLMï¼‰
        
        Args:
            image_path (str): åˆ¤å®šå¯¾è±¡ç”»åƒ
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            page_number (int): ãƒšãƒ¼ã‚¸ç•ªå·
            image_index (Optional[int]): ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè¤‡æ•°ç”»åƒã®å ´åˆï¼‰
            
        Returns:
            Dict: åˆ¤å®šçµæœ
        """
        try:
            prompts = self.prompts.get("page_count_etc_judgment", {})
            # Fix: evaluate_page_count only expects (image_path, prompts)
            result = self.llm_evaluator_judgment.evaluate_page_count(image_path, prompts)
            
            # çµæœã‚’ä¿å­˜ï¼ˆè¤‡æ•°ç”»åƒã®å ´åˆã®ä¸Šæ›¸ãå›é¿ã®ãŸã‚ image_index ã‚’ä»˜ä¸ï¼‰
            if result["success"]:
                if image_index is not None:
                    output_file = os.path.join(output_dir, f"page_{page_number:03d}_page_count_img{image_index+1}.json")
                else:
                    output_file = os.path.join(output_dir, f"page_{page_number:03d}_page_count.json")
                self.llm_evaluator_judgment.save_result(result, output_file)
        
            return result
            
        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸æ•°ç­‰åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def _apply_page_count_etc_judgment(self, page_judgments: List[Dict], session_dirs: Dict[str, str]) -> Dict[str, Dict]:
        """
        ã‚¹ãƒ†ãƒƒãƒ—4-1: ãƒšãƒ¼ã‚¸æ•°ç­‰åˆ¤å®šï¼ˆåˆ†å‰²å‰ã€å¿…è¦ã«å¿œã˜ã¦è¤‡æ•°ç”»åƒã‚’é›†ç´„ï¼‰
        å„ãƒšãƒ¼ã‚¸ã®ä»£è¡¨/æ´¾ç”Ÿç”»åƒã«å¯¾ã—ã¦åˆ¤å®šã‚’å®Ÿæ–½ã—ã€ãƒãƒ¼ã‚¸ã—ãŸçµæœã‚’è¿”ã™ã€‚

        Returns:
            Dict[str, Dict]: {"page_XXX_page_count": {merged, individual, success}}
        """
        results_map: Dict[str, Dict] = {}
        for page_data in page_judgments:
            pn = page_data["page_number"]
            proc_images = page_data.get("processed_images") or [page_data.get("processed_image")]
            logger.info(f"ğŸ” ã‚¹ãƒ†ãƒƒãƒ—4-1: LLMãƒšãƒ¼ã‚¸æ•°åˆ¤å®š ãƒšãƒ¼ã‚¸{pn} ({len(proc_images)}ç”»åƒ)")
            individual_results: List[Dict] = []
            for idx, img_path in enumerate(proc_images):
                r = self._page_count_etc_llm_judgment(img_path, session_dirs["llm_judgments"], pn, image_index=(idx if len(proc_images) > 1 else None))
                individual_results.append(r)

            # ãƒãƒ¼ã‚¸ãƒ­ã‚¸ãƒƒã‚¯
            bool_or_fields = ["has_table_elements", "has_handwritten_notes_or_marks"]
            merged_bools: Dict[str, str] = {}
            for key in bool_or_fields:
                acc = False
                for res in individual_results:
                    j = (res or {}).get("judgment", {})
                    if key in j:
                        acc = acc or self._to_bool(j.get(key))
                merged_bools[key] = "True" if acc else "False"

            # page_count ã¯åŠ ç®—ã—ã€æœ€å¤§3ã«ã‚¯ãƒ©ãƒ³ãƒ—
            merged_page_count = 0
            page_count_conf_list: List[float] = []
            conf_list: List[float] = []
            readability_comments: List[str] = []
            overall_comments: List[str] = []
            order = {"none": 0, "minor": 1, "major": 2}
            rev_order = {v: k for k, v in order.items()}
            worst_val = -1
            for i, res in enumerate(individual_results, 1):
                j = (res or {}).get("judgment", {})
                pc = self._to_int(j.get("page_count"))
                try:
                    merged_page_count += pc if pc is not None else 0
                except Exception:
                    pass
                pc_conf = self._to_float(j.get("page_count_confidence"))
                if pc_conf is not None:
                    page_count_conf_list.append(pc_conf)
                conf_v = self._to_float(j.get("confidence_score"))
                if conf_v is not None:
                    conf_list.append(conf_v)
                rc = j.get("readability_comment")
                if rc:
                    readability_comments.append(f"page{i}ã®ã‚³ãƒ¡ãƒ³ãƒˆ: {rc}")
                oc = j.get("overall_comment")
                if oc:
                    overall_comments.append(f"page{i}ã®ã‚³ãƒ¡ãƒ³ãƒˆ: {oc}")
                ri = str(j.get("readability_issues", "")).lower()
                if ri in order:
                    worst_val = max(worst_val, order[ri])

            # ã‚¯ãƒ©ãƒ³ãƒ—
            if merged_page_count <= 0:
                merged_page_count = 1
            if merged_page_count > 3:
                merged_page_count = 3
            avg_pc_conf = sum(page_count_conf_list) / len(page_count_conf_list) if page_count_conf_list else None
            avg_conf = sum(conf_list) / len(conf_list) if conf_list else None

            merged_judgment = {
                **merged_bools,
                "page_count": merged_page_count,
                "page_count_confidence": round(avg_pc_conf, 3) if avg_pc_conf is not None else None,
                "confidence_score": round(avg_conf, 3) if avg_conf is not None else None,
                "readability_issues": rev_order.get(worst_val, "none") if worst_val >= 0 else "none",
                "readability_comment": "\n".join(readability_comments) if readability_comments else None,
                "overall_comment": "\n".join(overall_comments) if overall_comments else None,
            }

            results_map[f"page_{pn:03d}_page_count"] = {
                "success": True,
                "merged": merged_judgment,
                "individual": individual_results,
            }

            # å¾Œç¶šå‡¦ç†ã®ãŸã‚ã« page_count ã‚’ä¿å­˜
            try:
                page_data["page_count"] = int(merged_judgment.get("page_count", 1))
            except Exception:
                page_data["page_count"] = 1

        return results_map

    def _apply_page_splits(self, page_judgments: List[Dict], session_dirs: Dict[str, str]):
        """
        ã‚¹ãƒ†ãƒƒãƒ—4-2: page_count=2 ã®å ´åˆã®å¼·åˆ¶å·¦å³åˆ†å‰²
        """
        logger.info("âœ‚ï¸ ã‚¹ãƒ†ãƒƒãƒ— 4-2: page_count=2 å¼·åˆ¶åˆ†å‰²ãƒã‚§ãƒƒã‚¯")
        for i, page_data in enumerate(page_judgments, 1):
            if page_data.get("page_count") == 2 and not page_data.get("skip_processing") and len(page_data.get("processed_images", [])) == 1:
                page_number = page_data["page_number"]
                logger.info(f"ğŸ”„ å¼·åˆ¶åˆ†å‰²å¯¾è±¡ ({i}/{len(page_judgments)}) ãƒšãƒ¼ã‚¸{page_number}")
                try:
                    image_to_split = page_data["processed_images"][0]
                    image = cv2.imread(image_to_split)
                    if image is None:
                        raise IOError(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_to_split}")

                    forced_split_output_dir = os.path.join(session_dirs["dewarped"], "forced_split")
                    os.makedirs(forced_split_output_dir, exist_ok=True)
                    base_filename = f"page_{page_number:03d}_forced"

                    left_path, right_path = split_image_left_right_with_overlap(
                        image=image, overlap_ratio=0.1, output_dir=forced_split_output_dir, base_filename=base_filename
                    )

                    page_data["processed_images"] = [left_path, right_path]
                    page_data["processed_image"] = left_path
                    page_data["forced_split_applied"] = True
                    logger.info(f"  â†’ âœ…å¼·åˆ¶åˆ†å‰²å®Œäº†: 2å€‹ã®ç”»åƒç”Ÿæˆ")
                except Exception as e:
                    logger.error(f"  â†’ âŒå¼·åˆ¶åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
                    page_data["forced_split_applied"] = False

    def _image_splitting_for_ocr(self, image_path: str, output_dir: str, page_number: int) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—5: ç”»åƒ5ç­‰åˆ†
        """
        logger.info(f"ãƒšãƒ¼ã‚¸ {page_number}: ç”»åƒåˆ†å‰²å‡¦ç†")
        try:
            base_name = f"page_{page_number:03d}"
            page_output_dir = os.path.join(output_dir, base_name)
            result = self.image_splitter.split_and_save(image_path, page_output_dir, base_name)
            return result
        except Exception as e:
            logger.error(f"ç”»åƒåˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def _apply_image_split_for_ocr(self, page_judgments: List[Dict], session_dirs: Dict[str, str]) -> List[Dict]:
        """
        ã‚¹ãƒ†ãƒƒãƒ—5: OCRç”¨ã®ç”»åƒåˆ†å‰²ï¼ˆæ­ªã¿è£œæ­£å¾Œã®å„ç”»åƒã«å¯¾ã—ã¦ï¼‰
        """
        logger.info("âœ‚ï¸ ã‚¹ãƒ†ãƒƒãƒ— 5: å…¨ãƒšãƒ¼ã‚¸ç”»åƒåˆ†å‰²ï¼ˆãƒšãƒ¼ã‚¸æ•°å¯¾å¿œï¼‰")
        all_split_images: List[Dict] = []
        for i, page_data in enumerate(page_judgments, 1):
            page_number = page_data["page_number"]
            logger.info(f"âœ‚ï¸ ç”»åƒåˆ†å‰² ({i}/{len(page_judgments)}) ãƒšãƒ¼ã‚¸{page_number}")
            processed_images = page_data.get("processed_images", [])
            split_results_by_source: List[Dict] = []
            for img_idx, proc_image_path in enumerate(processed_images):
                if len(processed_images) > 1:
                    logger.info(f"  ğŸ“„ æ­ªã¿è£œæ­£ç”»åƒ {img_idx + 1}/{len(processed_images)} ã‚’åˆ†å‰²å‡¦ç†")
                base_name = f"page_{page_number:03d}_mask{img_idx + 1}"
                split_output_dir = os.path.join(session_dirs["split_images"], base_name)
                split_result = self._image_splitting_for_ocr(proc_image_path, split_output_dir, page_number)
                split_result["source_dewarped_image"] = proc_image_path
                split_result["source_mask_index"] = img_idx
                split_results_by_source.append(split_result)

            page_data["split_results_by_source"] = split_results_by_source

            total_split_images = 0
            for source_idx, split_result in enumerate(split_results_by_source):
                if split_result.get("success"):
                    split_images = split_result.get("split_paths", [])
                    total_split_images += len(split_images)
                    for img_path in split_images:
                        all_split_images.append({
                            "page_number": page_number, "image_path": img_path, "image_type": "split",
                            "source_mask_index": source_idx, "source_dewarped_image": split_result["source_dewarped_image"]
                        })
            logger.info(f"  â†’ âœ…åˆè¨ˆ{total_split_images}å€‹ã®åˆ†å‰²ç”»åƒç”Ÿæˆ")

            # å…ƒç”»åƒï¼ˆæ­ªã¿è£œæ­£å¾Œã®å„ç”»åƒï¼‰ã‚’OCRã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ã‚¤ãƒ³ç”»åƒã¨ã—ã¦è¿½åŠ 
            for source_idx, split_result in enumerate(split_results_by_source):
                if split_result.get("success"):
                    original_path = split_result.get("original_path")
                    if original_path:
                        all_split_images.append({
                            "page_number": page_number, "image_path": original_path, "image_type": "original",
                            "source_mask_index": source_idx, "source_dewarped_image": split_result["source_dewarped_image"]
                        })
        return all_split_images

    def _group_images_for_ocr(self, all_processed_images: List[Dict]) -> List[Dict]:
        """è¶…è§£åƒæ¸ˆã¿ã®ç”»åƒã‚’OCRã‚¸ãƒ§ãƒ–ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¾ã¨ã‚ã‚‹"""
        groups: Dict[Tuple[int,int], Dict] = {}
        for img_info in all_processed_images:
            page_number = img_info["page_number"]
            mask_index = img_info.get("source_mask_index", -1)
            group_key = (page_number, mask_index)
            if group_key not in groups:
                groups[group_key] = {
                    "page_number": page_number,
                    "mask_index": mask_index,
                    "original_image": None,
                    "split_images": []
                }
            image_type = img_info["image_type"]
            output_path = img_info.get("output_path", img_info["image_path"])
            if image_type == "original":
                groups[group_key]["original_image"] = output_path
            elif image_type == "split":
                groups[group_key]["split_images"].append(output_path)
        ocr_jobs: List[Dict] = []
        for group_key, group_data in sorted(groups.items()):
            image_paths: List[str] = []
            if group_data["original_image"]:
                image_paths.append(group_data["original_image"])
            image_paths.extend(sorted(group_data["split_images"]))
            ocr_jobs.append({
                "page_number": group_data["page_number"],
                "mask_index": group_data["mask_index"],
                "image_paths": image_paths
            })
        return ocr_jobs

    def _create_skip_super_resolution_result(self, phase1_result: Dict) -> Dict:
        """
        è¶…è§£åƒã‚¹ã‚­ãƒƒãƒ—æ™‚ã®çµæœã‚’ä½œæˆï¼ˆå…ƒç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
        """
        all_images = phase1_result.get("all_images_for_sr", [])
        processed_images: List[Dict] = []
        for img_info in all_images:
            processed_images.append({
                **img_info,
                "success": True,
                "skipped": True,
                "skip_reason": "super_resolution_disabled",
                "output_path": img_info["image_path"]
            })
        logger.info(f"âš¡ è¶…è§£åƒã‚¹ã‚­ãƒƒãƒ—: {len(all_images)}å€‹ã®ç”»åƒã‚’å…ƒç”»åƒã®ã¾ã¾ä½¿ç”¨")
        return {
            "success": True,
            "total_images_processed": len(all_images),
            "successful_sr": 0,
            "skipped": True,
            "all_processed_images": processed_images
        }

    def _batch_super_resolution(self, phase1_result: Dict, session_dirs: Dict[str, str]) -> Dict:
        """
        ãƒ•ã‚§ãƒ¼ã‚º2: å…¨ç”»åƒã®ãƒãƒƒãƒè¶…è§£åƒå‡¦ç†
        
        Args:
            phase1_result (Dict): ãƒ•ã‚§ãƒ¼ã‚º1ã®çµæœ
            session_dirs (Dict[str, str]): ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            Dict: è¶…è§£åƒå‡¦ç†çµæœ
        """
        logger.info("ğŸ” === ãƒ•ã‚§ãƒ¼ã‚º2: ãƒãƒƒãƒè¶…è§£åƒå‡¦ç† ===")
        
        # è¶…è§£åƒå‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹å ´åˆ
        if not self.config.get('super_resolution', {}).get('enabled', True):
            logger.info("âš¡ è¶…è§£åƒå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return self._create_skip_super_resolution_result(phase1_result)
        
        all_images = phase1_result.get("all_images_for_sr", [])
        split_images_to_process = [img for img in all_images if img["image_type"] == "split"]
        
        logger.info(f"ğŸ¯ è¶…è§£åƒå¯¾è±¡: {len(split_images_to_process)} å€‹ã®åˆ†å‰²ç”»åƒ")
        
        sr_results = []
        success_count = 0
        
        for i, img_info in enumerate(split_images_to_process, 1):
            page_number = img_info["page_number"]
            image_path = img_info["image_path"]
            
            image_name = os.path.splitext(os.path.basename(image_path))[0]
            # ãƒã‚¹ã‚¯æƒ…å ±ã«åŸºã¥ã„ã¦å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
            source_mask_index = img_info.get("source_mask_index", 0)
            base_name = f"page_{page_number:03d}_mask{source_mask_index + 1}"
            output_dir = os.path.join(session_dirs["super_resolved"], base_name)
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, f"{image_name}_sr.jpg")

            try:
                if hasattr(self.sr_runner, 'clear_cuda_cache'):
                    self.sr_runner.clear_cuda_cache()
                
                logger.info(f"ğŸ”„ è¶…è§£åƒ ({i}/{len(split_images_to_process)}) ãƒšãƒ¼ã‚¸{page_number} - {os.path.basename(image_path)}")
                sr_result = self.sr_runner.process_image(image_path, output_path)
                
                # å…ƒã®ç”»åƒæƒ…å ±ã‚’å¼•ãç¶™ã
                sr_result.update(img_info)
                sr_results.append(sr_result)
                
                if sr_result["success"]:
                    success_count += 1
                    if not sr_result.get("skipped"):
                        logger.info(f"  â†’ âœ…è¶…è§£åƒæˆåŠŸ")
                else:
                    logger.warning(f"  â†’ âŒè¶…è§£åƒå¤±æ•—: {sr_result.get('error', 'unknown')}")
                    shutil.copy2(image_path, output_path)

            except Exception as e:
                logger.error(f"  â†’ ğŸ’¥è¶…è§£åƒã‚¨ãƒ©ãƒ¼: {e}")
                shutil.copy2(image_path, output_path)
                sr_results.append({
                    **img_info,
                    "success": False, "error": str(e), "fallback_copy": True
                })

        # è¶…è§£åƒã•ã‚Œãªã‹ã£ãŸã€Œã‚ªãƒªã‚¸ãƒŠãƒ«ã€ç”»åƒã®æƒ…å ±ã‚‚çµæœã«å«ã‚ã‚‹
        original_images = [img for img in all_images if img["image_type"] == "original"]
        for img_info in original_images:
            sr_results.append({
                **img_info,
                "success": True, "skipped": True, "skip_reason": "original_image_not_processed",
                "output_path": img_info["image_path"]
            })

        result = {
            "success": True,
            "total_images_processed": len(split_images_to_process),
            "successful_sr": success_count,
            "all_processed_images": sr_results
        }
        
        logger.info(f"âœ… ãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†: è¶…è§£åƒ{success_count}/{len(split_images_to_process)}æˆåŠŸ")
        return result

    def _batch_ocr(self, phase2_result: Dict, session_dirs: Dict[str, str]) -> Dict:
        """
        ãƒ•ã‚§ãƒ¼ã‚º3: ãƒãƒƒãƒOCRå‡¦ç†
        
        Args:
            phase2_result (Dict): ãƒ•ã‚§ãƒ¼ã‚º2ã®çµæœ
            session_dirs (Dict[str, str]): ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            Dict: OCRå‡¦ç†çµæœ
        """
        logger.info("ğŸ“ === ãƒ•ã‚§ãƒ¼ã‚º3: ãƒãƒƒãƒOCRå‡¦ç† ===")
        
        # OCRå‡¦ç†ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹å ´åˆ
        if not self.config.get('llm_evaluation', {}).get('ocr_enabled', True):
            logger.info("âš¡ OCRå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return self._create_skip_ocr_result(phase2_result)
        
        all_processed_images = phase2_result.get("all_processed_images", [])
        
        # OCRã‚¸ãƒ§ãƒ–ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        ocr_jobs = self._group_images_for_ocr(all_processed_images)
        
        # å¥‘ç´„æ›¸ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueï¼‰
        is_contract_mode = self.config.get('ocr', {}).get('contract_mode', True)
        
        ocr_results = []
        
        if is_contract_mode:
            # å¥‘ç´„æ›¸ãƒ¢ãƒ¼ãƒ‰ï¼šå…¨ãƒšãƒ¼ã‚¸ã‚’ä¸€åº¦ã«å‡¦ç†
            logger.info(f"ğŸ“ å¥‘ç´„æ›¸OCR: å…¨{len(ocr_jobs)}ã‚¸ãƒ§ãƒ–ã‚’çµ±åˆå‡¦ç†")
            
            # å…¨ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’åé›†
            all_images = []
            for job in ocr_jobs:
                all_images.extend(job["image_paths"])
            
            logger.info(f"    ğŸ“Š å…¨ç”»åƒæ•°: {len(all_images)}æš")
            
            try:
                if not all_images:
                    raise ValueError("OCRå¯¾è±¡ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
                
                # å…¨ãƒšãƒ¼ã‚¸ã‚’ä¸€åº¦ã«å¥‘ç´„æ›¸OCRå‡¦ç†
                ocr_prompts = self.prompts.get("multi_image_ocr", self.prompts.get("ocr_extraction", {}))
                contract_result = self.llm_evaluator_ocr.extract_contract_ocr_multi_images(all_images, ocr_prompts)
                
                contract_result.update({
                    "page_count": len(ocr_jobs),
                    "total_images": len(all_images),
                    "processing_mode": "contract_unified"
                })
                ocr_results.append(contract_result)
                
                if contract_result.get("success"):
                    logger.info(f"    â†’ âœ…å¥‘ç´„æ›¸OCRçµ±åˆå‡¦ç†æˆåŠŸ")
                else:
                    logger.warning(f"    â†’ âŒå¥‘ç´„æ›¸OCRçµ±åˆå‡¦ç†å¤±æ•—: {contract_result.get('error', 'unknown')}")
                    
            except Exception as e:
                logger.error(f"    â†’ ğŸ’¥å¥‘ç´„æ›¸OCRã‚¨ãƒ©ãƒ¼: {e}")
                ocr_results.append({
                    "success": False, "error": str(e),
                    "processing_mode": "contract_unified"
                })
        
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šãƒšãƒ¼ã‚¸ã”ã¨ã«å‡¦ç†
            for i, job in enumerate(ocr_jobs, 1):
                page_number = job["page_number"]
                mask_index = job["mask_index"]
                image_paths = job["image_paths"]
                
                logger.info(f"ğŸ“ OCRã‚¸ãƒ§ãƒ– ({i}/{len(ocr_jobs)}) ãƒšãƒ¼ã‚¸{page_number}, ãƒã‚¹ã‚¯{mask_index + 1}")
                logger.info(f"    ğŸ“Š OCRå¯¾è±¡: {len(image_paths)}æš")

                try:
                    if not image_paths:
                        raise ValueError("OCRå¯¾è±¡ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")

                    # é€šå¸¸ã®OCR
                    ocr_prompts = self.prompts.get("multi_image_ocr", self.prompts.get("ocr_extraction", {}))
                    ocr_result = self.llm_evaluator_ocr.extract_text_ocr_multi_images(image_paths, ocr_prompts)
                    
                    ocr_result.update({
                        "page_number": page_number,
                        "mask_index": mask_index,
                        "group_type": "dewarped_mask" if mask_index != -1 else "no_dewarping",
                        "num_images": len(image_paths)
                    })
                    ocr_results.append(ocr_result)

                    if ocr_result.get("success"):
                        logger.info(f"    â†’ âœ…OCRæˆåŠŸ")
                    else:
                        logger.warning(f"    â†’ âŒOCRå¤±æ•—: {ocr_result.get('error', 'unknown')}")

                except Exception as e:
                    logger.error(f"    â†’ ğŸ’¥OCRã‚¨ãƒ©ãƒ¼: {e}")
                    ocr_results.append({
                        "success": False, "error": str(e),
                        "page_number": page_number, "mask_index": mask_index
                    })

        successful_ocr = len([r for r in ocr_results if r.get("success")])
        result = {
            "success": successful_ocr > 0,
            "total_jobs": len(ocr_jobs),
            "successful_ocr": successful_ocr,
            "ocr_results": ocr_results
        }
        
        logger.info(f"âœ… ãƒ•ã‚§ãƒ¼ã‚º3å®Œäº†: {successful_ocr}/{len(ocr_jobs)}ã‚¸ãƒ§ãƒ–æˆåŠŸ")
        return result
    
    def _save_pipeline_result(self, result: Dict, output_dir: str):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‚’JSONä¿å­˜"""
        try:
            result_path = os.path.join(output_dir, "pipeline_result.json")
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä¿å­˜: {os.path.basename(result_path)}")
        except Exception as e:
            logger.error(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _cleanup_temp_files(self, session_dirs: Dict[str, str]):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not self.config.get('debug', {}).get('skip_cleanup', False):
            temp_dir = self.config.get('system', {}).get('temp_dir')
            if temp_dir and os.path.exists(temp_dir):
                cleanup_directory(temp_dir)
                logger.info("ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

        # === è¿½åŠ : LLMå‡ºåŠ›ã®å‹è§£é‡ˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¯ãƒ©ã‚¹å…¨ä½“ã§åˆ©ç”¨å¯èƒ½ã« ===

    def process_pdf(self, pdf_path: str, output_session_id: Optional[str] = None) -> Dict:
        """
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            pdf_path (str): å‡¦ç†å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_session_id (str, optional): å‡ºåŠ›ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            
        Returns:
            Dict: å‡¦ç†çµæœã®è©³ç´°æƒ…å ±
        """
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ç”Ÿæˆ
        if output_session_id is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(os.path.basename(pdf_path))[0]
            output_session_id = f"{base_name}_{timestamp}"
        
        logger.info(f"ğŸš€ === PDFå‡¦ç†é–‹å§‹: {os.path.basename(pdf_path)} ===")
        logger.info(f"ğŸ·ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {output_session_id}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        session_dirs = self._create_session_directories(output_session_id)
        
        # å‡¦ç†çµæœã‚’è¨˜éŒ²
        pipeline_result = {
            "session_id": output_session_id,
            "input_pdf": pdf_path,
            "start_time": datetime.now().isoformat(),
            "session_dirs": session_dirs,
            "steps": {
                "llm_judgments": {},
            },
            "final_results": {},
            "success": False
        }
        
        try:
            # ç¾åœ¨ã®PDFãƒ‘ã‚¹ã‚’ä¿å­˜
            self._current_pdf_path = pdf_path

            # ã‚¹ãƒ†ãƒƒãƒ—1: PDF â†’ JPGå¤‰æ›
            logger.info("ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—1: PDF â†’ JPGå¤‰æ›")
            pdf_result = self._pdf_to_jpg(pdf_path, session_dirs["converted_images"])
            pipeline_result["steps"]["pdf_conversion"] = pdf_result
            # PDFå¤‰æ›çµæœã‚’ä¿å­˜ï¼ˆå„ãƒšãƒ¼ã‚¸ã®DPIæƒ…å ±ã‚’å«ã‚€ï¼‰
            self._current_pdf_info = pdf_result
            
            if not pdf_result.get("success"):
                raise RuntimeError("PDFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")

            # ãƒšãƒ¼ã‚¸é…åˆ—
            pages = [p for p in pdf_result.get("pages", []) if p.get("success")]
            if not pages:
                raise RuntimeError("å¤‰æ›æˆåŠŸãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")

            # å„ãƒšãƒ¼ã‚¸ã®åˆæœŸãƒ‡ãƒ¼ã‚¿
            page_judgments: List[Dict] = []
            for p in pages:
                pn = p["page_number"]
                orig_img = p["image_file"]
                page_rec = {
                    "page_number": pn,
                    "original_image": orig_img,
                    "processed_image": orig_img,
                    "processed_images": [orig_img],
                    "skip_processing": False
                }
                # ã‚¹ãƒ†ãƒƒãƒ—2-1: æ­ªã¿åˆ¤å®šï¼ˆè¨˜éŒ²ã®ã¿ã€åˆ¤æ–­ã¯å¾Œç¶šã§ä½¿ç”¨ï¼‰
                logger.info(f"ğŸ” ã‚¹ãƒ†ãƒƒãƒ—2-1: LLMæ­ªã¿åˆ¤å®š ãƒšãƒ¼ã‚¸{pn}")
                judge_res = self._dewarping_llm_judgment(orig_img, session_dirs["llm_judgments"], pn)
                pipeline_result["steps"]["llm_judgments"][f"page_{pn:03d}_dewarp"] = judge_res
                j = (judge_res or {}).get("judgment", {})
                needs_dewarping = self._to_bool(j.get("needs_dewarping")) or self._to_bool(j.get("has_something_out_of_document"))
                readability_issues = str(j.get("readability_issues", "")).lower()
                page_rec["readability_issues"] = readability_issues
                page_rec["reprocessed_at_scale"] = False
                page_rec["needs_dewarping"] = needs_dewarping
                page_judgments.append(page_rec)
        
            # ã‚¹ãƒ†ãƒƒãƒ—2-2: 2xå†ç”»åƒåŒ–(èª­ã¿ã«ãã•ãŒ majorã®å ´åˆ)
            self._apply_reprocess_page(page_judgments, session_dirs)

            # ã‚¹ãƒ†ãƒƒãƒ—2-3: æ­ªã¿è£œæ­£ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            self._apply_dewarping(page_judgments, session_dirs)

            # ã‚¹ãƒ†ãƒƒãƒ—3-1: å›è»¢åˆ¤å®šãŠã‚ˆã³è£œæ­£
            self._apply_orientation_detector(page_judgments, session_dirs)

            # ã‚¹ãƒ†ãƒƒãƒ—4-1: ãƒšãƒ¼ã‚¸æ•°ç­‰åˆ¤å®š
            pagecount_results = self._apply_page_count_etc_judgment(page_judgments, session_dirs)
            pipeline_result["steps"]["llm_judgments"].update(pagecount_results)

            # ã‚¹ãƒ†ãƒƒãƒ—4-2: page_count=2 ã®å ´åˆã®å¼·åˆ¶å·¦å³åˆ†å‰²
            self._apply_page_splits(page_judgments, session_dirs)

            # ã‚¹ãƒ†ãƒƒãƒ—5: OCRç”¨ã®ç”»åƒåˆ†å‰²
            all_images_for_sr = self._apply_image_split_for_ocr(page_judgments, session_dirs)

            # ã‚¹ãƒ†ãƒƒãƒ—6: è¶…è§£åƒï¼ˆè¨­å®šã«ã‚ˆã‚‹ã‚¹ã‚­ãƒƒãƒ—å¯¾å¿œï¼‰
            phase1_like = {"all_images_for_sr": all_images_for_sr}
            if not self.config.get('super_resolution', {}).get('enabled', True):
                sr_result = self._create_skip_super_resolution_result(phase1_like)
            else:
                sr_result = self._batch_super_resolution(phase1_like, session_dirs)
            pipeline_result["steps"]["super_resolution"] = sr_result

            # ã‚¹ãƒ†ãƒƒãƒ—7: OCRï¼ˆè¨­å®šã«ã‚ˆã‚‹ã‚¹ã‚­ãƒƒãƒ—å¯¾å¿œï¼‰
            if not self.config.get('llm_evaluation', {}).get('ocr_enabled', True):
                ocr_result = self._create_skip_ocr_result(sr_result)
            else:
                ocr_result = self._batch_ocr(sr_result, session_dirs)
            pipeline_result["final_results"] = ocr_result

            pipeline_result["success"] = True
            pipeline_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            pipeline_result["error"] = str(e)
            pipeline_result["end_time"] = datetime.now().isoformat()
        
        # çµæœã‚’ä¿å­˜
        self._save_pipeline_result(pipeline_result, session_dirs["final_results"])
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if self.config.get('system', {}).get('cleanup_temp', True):
            self._cleanup_temp_files(session_dirs)
        
        return pipeline_result
    
def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Document OCR Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
            ä½¿ç”¨ä¾‹:
            python main_pipeline.py --input document.pdf
            python main_pipeline.py --input document.pdf --config config/config_test.yaml
            python main_pipeline.py --input document.pdf --session-id my_session
                    """
                )
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®çµ¶å¯¾ãƒ‘ã‚¹è¨­å®š
    project_root = Path(__file__).parent.parent.parent
    default_config = project_root / "config" / "config.yaml"
    
    parser.add_argument(
        "--config", 
        default=str(default_config), 
        help=f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {default_config})"
    )
    parser.add_argument(
        "--input",
        help="å…¥åŠ›PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œï¼‰"
    )
    parser.add_argument(
        "--session-id", 
        help="ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•ç”Ÿæˆï¼‰"
    )
    parser.add_argument(
        "--sample",
        action="store_true",
        help="ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"
    )
    parser.add_argument(
        "--check-config",
        action="store_true",
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦çµ‚äº†"
    )
    
    args = parser.parse_args()
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(args.config):
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å¯¾å‡¦
            config_test_path = project_root / "config" / "config_test.yaml"
            if config_test_path.exists():
                print(f"âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.config}")
                print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {config_test_path}")
                args.config = str(config_test_path)
            else:
                print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.config}")
                print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«:")
                config_dir = project_root / "config"
                if config_dir.exists():
                    for config_file in config_dir.glob("*.yaml"):
                        print(f"   - {config_file}")
                else:
                    print(f"   è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {config_dir}")
                print(f"\nğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã‚’è©¦ã™å ´åˆ: python {Path(__file__).name} --sample")
                return 1
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        pipeline = DocumentOCRPipeline(args.config)
        
        # è¨­å®šãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰
        if args.check_config:
            print("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            print("âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–æˆåŠŸ")
            print("ğŸ“‹ è¨­å®šå†…å®¹:")
            for component, config in pipeline.config.items():
                if isinstance(config, dict):
                    print(f"   {component}: {len(config)} items")
                else:
                    print(f"   {component}: {config}")
            return 0
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
        if args.sample:
            # ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
            print("ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™")
            input_path = _create_sample_pdf(project_root)
            if not input_path:
                print("âŒ ã‚µãƒ³ãƒ—ãƒ«PDFä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return 1
            print(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«PDFä½œæˆ: {input_path}")
        elif args.input:
            # æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨
            input_path = args.input
            if not os.path.exists(input_path):
                print(f"âŒ å…¥åŠ›PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
                print(f"ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã‚’è©¦ã™å ´åˆ: python {Path(__file__).name} --sample")
                return 1
        else:
            # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•æ¤œç´¢
            input_dir = project_root / "data" / "input"
            if input_dir.exists():
                pdf_files = list(input_dir.glob("*.pdf"))
                if pdf_files:
                    input_path = str(pdf_files[0])
                    print(f"ğŸ“„ è‡ªå‹•æ¤œå‡ºPDFãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(input_path)}")
                else:
                    print(f"âŒ data/input/ ã«PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    print(f"ğŸ’¡ ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è©¦ã—ã¦ãã ã•ã„:")
                    print(f"   - PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ data/input/ ã«é…ç½®")
                    print(f"   - --input ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æŒ‡å®š")
                    print(f"   - --sample ã§ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ")
                    return 1
            else:
                print(f"âŒ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_dir}")
                print(f"ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã‚’è©¦ã™å ´åˆ: python {Path(__file__).name} --sample")
                return 1
        
        print(f"ğŸš€ PDFå‡¦ç†é–‹å§‹: {os.path.basename(input_path)}")
        
        # PDFå‡¦ç†å®Ÿè¡Œ
        result = pipeline.process_pdf(input_path, args.session_id)
        
        # çµæœè¡¨ç¤º
        if result["success"]:
            print(f"âœ… å‡¦ç†å®Œäº†: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID {result['session_id']}")
            final_results = result.get("final_results", {})
            if final_results.get("success"):
                print(f"ğŸ“ æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ: {final_results['final_text_path']}")
                print(f"ğŸ“Š å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {len(final_results['successful_pages'])}")
            else:
                print(f"âš ï¸ ä¸€éƒ¨å‡¦ç†ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€éƒ¨åˆ†çš„ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            print(f"âŒ å‡¦ç†å¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            return 1
            
        return 0
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¡ è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return 1

def _create_sample_pdf(project_root: Path) -> Optional[str]:
    """
    ã‚µãƒ³ãƒ—ãƒ«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    
    Args:
        project_root (Path): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
        
    Returns:
        Optional[str]: ä½œæˆã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    try:
        from PIL import Image, ImageDraw
        import tempfile
        
        # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒä½œæˆ
        img = Image.new('RGB', (2100, 2970), color='white')  # A4ã‚µã‚¤ã‚ºç›¸å½“
        draw = ImageDraw.Draw(img)
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        draw.text((100, 100), "Document OCR Pipeline Test PDF", fill='black')
        draw.text((100, 200), "Generated for main_pipeline.py testing", fill='gray')
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        y_pos = 400
        sample_content = [
            "Chapter 1: Introduction",
            "",
            "This is a sample PDF document created for testing",
            "the Document OCR Pipeline system. The system",
            "processes PDF files through the following steps:",
            "",
            "1. PDF to JPG conversion with DPI optimization",
            "2. LLM-based distortion judgment",
            "3. YOLO-based document dewarping",
            "4. Image split_image_5parts with overlap",
            "5. DRCT super-resolution processing",
            "6. Multi-image batch OCR",
            "7. Final text integration",
            "",
            "Chapter 2: Test Content",
            "",
            "Lorem ipsum dolor sit amet, consectetur",
            "adipiscing elit. Sed do eiusmod tempor",
            "incididunt ut labore et dolore magna aliqua.",
            "",
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        ]
        
        for line in sample_content:
            draw.text((100, y_pos), line, fill='black')
            y_pos += 60
        
        # æ ç·š
        draw.rectangle([50, 50, 2050, 2920], outline='black', width=3)
        
        # ä¸€æ™‚ç”»åƒã¨ã—ã¦ä¿å­˜
        input_dir = project_root / "data" / "input"
        input_dir.mkdir(parents=True, exist_ok=True)
        
        sample_path = input_dir / "sample_test.jpg"
        img.save(sample_path, 'JPEG', quality=95)
        
        # JPGã‹ã‚‰PDFã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        pdf_path = input_dir / "sample_test.pdf"
        img_pdf = img.convert('RGB')
        img_pdf.save(pdf_path, 'PDF')
        
        return str(pdf_path)
        
    except Exception as e:
        print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«PDFä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    import sys
    sys.exit(main())